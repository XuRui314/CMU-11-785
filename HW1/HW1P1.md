&ensp; In this HW, i'm gonna to complete the basic MLP (aka the simplest NN) by designing my own `mytorch` library which can be reused in the subsequent HW. 

&ensp;Â Here is the layout of this note (blog):

1. Brief Introduction
2. Python Implementation
3. Torch Pipeline

## Introduction

### Representation

&ensp;Â MLPs are universal function approximators , they can model any Boolean function, classification function, or regression. Now, I will explain this powerful representation ability in an intuitive way.

&ensp;Â First, MLPs can be viewed as universal Boolean functions, because perceptrons can model`AND `, `OR`, `NOT` gate. Any truth table can be expressed in one-hidden-layer MLP, if we just get every item from the table and compose an expression, it costs $2^{N-1}$ neurons.

&ensp;Â But what if using more layers instead of just one-hidden-layer, by increasing the depth of the network, we are able to use less perceptrons, because it captures the relations between neurons.

&ensp;Â The decision boundary captured by the boolean network can be any convex region, and can be composed together by adding one more layer to get a bigger maybe non-convex region. In this way, we can get any region we want in theory. 

<img src=".\image\HW1P1_image1.png" alt="image-20221215153529805" style="zoom:80%;" />

&ensp;But can one-hidden-layer MLP model all the decision boundary, in another word, is one-hidden-layer MLP a universal classifier?

&ensp;Â The answer is YES, the intuition behind this is as follows. Consider now we want to model $\sum_{i=1}^Ny_i\geq N$, when we push $N$ to infinite, we will get a circle region in 2d, and a cylinder in 3d.

<img src=".\image\HW1P1_image2.png" alt="image-20221215153609086" style="zoom:80%;" />

&ensp; So in theory, we can achieve this:

<img src=".\image\HW1P1_image3.png" alt="image-20221215153753410" style="zoom:80%;" />

&ensp;Â But this is true only when N is infinite, this actually won't work in practice. However, deeper networks can require far fewer neuronsâ€“ 12 vs. ~infinite hidden neurons in this example. And in many cases, increase the depth can help a lot, a special case for this is the sum-product problem, also can be viewed as kind of DP idea.

&ensp;Â How can MLP model any function? The intuition is as behind:

<img src=".\image\HW1P1_image4.png" alt="image-20221215154409190" style="zoom:80%;" />



<img src=".\image\HW1P1_image5.png" alt="image-20221215154523328" style="zoom:80%;" />

&ensp;Â The network is actually a universal map from the entire domain of input values to the entire range of the output activation

&ensp;Â Next i will cover the trade-off between width, depth and sufficiency of architecture. Not all architectures can represent any function. If your layers width is narrow, and without proper activation function, the network may be not sufficient to represent some functions. However, narrow layers can still pass information to subsequent layers if the activation function is sufficiently graded. But will require greater depth, to permit later layers to capture patterns.

&ensp; In a short summary, deeper MLPs can achieve the same precision with far fewer neurons, but must still have sufficient capacity:

- The activations must pass information through 

- Each layer must still be sufficiently wide to convey all relevant information to subsequent layers.

&ensp; There are some supplement material analyzing the "capacity" of a network using VC dimension. I don't want to cover them all here. :)

&ensp; Then here let me introduce a variant network called RBF network. If you know Kernel SVM before, you must be familiar with the word RBF. So a RBF network is like this:

<img src=".\image\HW1P1_image6.png" alt="image-20221215183217223" style="zoom:80%;" />



&ensp; The difference between RBF network and BP network lies on the way they combine weights and inputs, see more here [link](https://stats.stackexchange.com/a/228596/363445).

&ensp; RBF network is the best approximation to continuous funcitons:

<img src=".\image\HW1P1_image9.png" style="zoom:80%;" />

&ensp; See more material on [RBFNN and SVM](https://www.cnblogs.com/pinking/p/9349695.html), [RBFNN and GMM](https://qr.ae/prpqSu)

### Learning theory

&ensp;Â We know that MLP can be constructed to represent any function, but there is a huge gap between "can" and "how to". One naive approach is to handcraft a network to satisfy it, but only for the simplest case. More generally, given the function to model, we want to derive the parameters of the network to model it, through computation. We learn networks (The network must have sufficient capacity to model the function) by â€œfittingâ€ them to training instances drawn from a target function. Estimate parameters to minimize the error between the target function $g(X)$ and the network function $f(X,W)$.

<img src=".\image\HW1P1_image10.png" alt="image-20221217160346417" style="zoom:80%;" />

&ensp;Â But $g(X)$ is unknow, we only get the sampled input-output pairs for a number of samples of input $X_i$ , $(X_i, d_i)$ , where $d_i = g(X_i) + noise$. We must learn the entire function from these few â€œtrainingâ€ samples.

<img src=".\image\HW1P1_image11.png" alt="image-20221217160601273" style="zoom:80%;" />

&ensp;Â For classification problem, there's an old method called perceptorn algorithm. So i'll only cover the main idea here, which is the update rule. Everytime we mis-classified a data point, we adjust our $W$ vector to fit this point. The detailed proof can be found in mit 6.036.

<img src=".\image\HW1P1_image12.png" alt="image-20221217161120280" style="zoom:80%;" />

<img src=".\image\HW1P1_image13.png" alt="image-20221217161323356" style="zoom:80%;" />

<img src=".\image\HW1P1_image14.png" alt="image-20221217161338657" style="zoom:80%;" />

&ensp;Â So can we apply the perceptron algorithm idea to the training process of MLP using the threshold function? The answer is NO. Even using the perfect architecture, it will still cost exponential time because we are using threshold function, so nobody tells us how far is it to the right answer, we have to try out every possible combinations.

&ensp;Â Suppose we get every perceptron right except for the yellow cricle one, we need to train it to get the line as follows:

<img src=".\image\HW1P1_image15.png" alt="image-20221217163051488" style="zoom:80%;" />

&ensp; The individual classifier actually requires the kind of labelling shown below which is not given. So we need to try out every possible way of relabeling the blue dots such that we can learn a line that keeps all the red dots on one side

<img src=".\image\HW1P1_image16.png" alt="image-20221217164916738" style="zoom:80%;" />

<img src=".\image\HW1P1_image17.png" alt="image-20221217165158501" style="zoom:80%;" />

<img src=".\image\HW1P1_image18.png" alt="image-20221217165331580" style="zoom:80%;" />

&ensp;Â So how to get rid of this limitation? In fact, it costs people more than a decade to give the solution XD. The problem is binary error metric is not useful, there is no indication of which direction to change the weights to reduce error, so we have to try out every possibility.

&ensp;Â The solution is to change our way of computing the mismatch such that modifying the classifier slightly lets us know if we are going the right way or no.

- This requires changing both, our activation functions, and the manner in which we evaluate the mismatch between the classifier output and the target output 
- Our mismatch function will now not actually count errors, but a proxy for it

&ensp;Â So we need our mismatch function to be differentiable. Small changes in weight can result in non-negligible changes in output. This enables us to estimate the parameters using gradient descent techniques.

&ensp;Â **Come back** to the problem of learning, we use divergence ( actually a Functional, take function as input, output a number ) to measure the mismatch, which is an abstract compared with the error defined before. Because when we are talking about "Error", this is often referred to the data-point-wise terminology, and we use "Total Error" to measure the overall dismatch. But when we step into the probability distribution world, this is not sufficient enough, so we proposed the concept of divergence, just an abstract of the "Error" we talked before. 

&ensp;Â Divergence is a functional, because we want to measure the difference between functions given the input. We don't really care about the input $X$ here, because we just use all the input values to calculate the divergence. We are more concerned about the relation between output of the divergence and the weights( which determines how our $f$ changes). 

<img src=".\image\HW1P1_image19.png" alt="image-20221217171018978" style="zoom:80%;" />

&ensp;More generally, assuming $X$ is a random variable, we don't need to consider the range we never cover, so we introduce the expectation to get the best $W$:
$$
\begin{aligned}
\widehat{\boldsymbol{W}}= & \underset{W}{\operatorname{argmin}} \int_X \operatorname{div}(f(X ; W), g(X)) P(X) d X \\
& =\underset{W}{\operatorname{argmin}} E[\operatorname{div}(f(X ; W), g(X))]
\end{aligned}
$$
&ensp;Â We used the concept "Risk" associated with hypothesis $h(x)$, which is defined as follows:
$$
R(h)={\mathbf  {E}}[L(h(x),y)]=\int L(h(x),y)\,dP(x,y).
$$
&ensp;Â In practice, we can only get few sampled data, so we need to define "Empirical risk":
$$
{\displaystyle \!R_{\text{emp}}(h)={\frac {1}{n}}\sum _{i=1}^{n}L(h(x_{i}),y_{i}).}
$$
<img src=".\image\HW1P1_image21.png" alt="image-20221217180102605" style="zoom:80%;" />

<img src=".\image\HW1P1_image20.png" alt="image-20221217194520685" style="zoom:80%;" />

&ensp;Its really a measure of error, but using standard terminology, we will call it a â€œLossâ€ . The empirical risk is only an empirical approximation to the true risk which is our actual minimization objective. For a given training set the loss is only a function of W.

&ensp;  Breif summary: We learn networks by â€œfittingâ€ them to training instances drawn from a target function. Learning networks of threshold-activation perceptrons requires solving a hard combinatorial-optimization problem.Because we cannot compute the influence of small changes to the parameters on the overall error. Instead we use continuous activation functions with non-zero derivatives to enables us to estimate network parameters. This makes the output of the network differentiable w.r.t every parameter in the networkâ€“ The logistic activation perceptron actually computes the a posteriori probability of the output given the input. We define differentiable divergence between the output of the network and the desired output for the training instances. And a total error, which is the average divergence over all training instances. We optimize network parameters to minimize this error--Empirical risk minimization. This is an instance of function minimization 

### Backpropagation

&ensp; In the previous part, we have talked about Empirical risk minimization, which is a kind of function minimization. So it's natural to use gradient decent to optimize the objective function. The big picture of training a network(no loops, no residual connections) is described as the following.Â 

<img src=".\image\HW1P1_image22.png" alt="image-20221224210413766" style="zoom:80%;" />

&ensp;As you see,  the first step of gradient decent is to calculate the gradient of the loss value with respect to the parameters. In this section, I will introduce backpropagatoin, which is a really efficient way to calculate gradient on the network.

&ensp;Â Our goal is to calculate $\frac{d \boldsymbol{D i v}(Y, \boldsymbol{d})}{d w_{i, j}^{(k)}}$ on the network, the naive approach is to just calculate one by one without using the properties of a network, which costs unacceptable computation. A more reasonable way is to take the idea of chain rule and dynamic programming into consideration, which is backpropagation.

&ensp;Here are the **assumptions** (All of these conditions are frequently not applicable): 

1. The computation of the output of one neuron does not directly affect computation of other neurons in the same (or previous) layers 
2. Inputs to neurons only combine through weighted addition 
3. Activations are actually differentiable  

&ensp;Â So now let's work on the math part. Actually, we just need to figure out one layer's  computation, since the other layers' calculations are actually similar.  I will choose to start from the end of the network.

<img src=".\image\HW1P1_image23.png" alt="image-20221224214406340" style="zoom:80%;" />

&ensp;Â Start from the grey box (loss function calculation):
$$
\frac{\partial \operatorname{Div}(Y, d)}{\partial y_i^{(N)}}=\frac{\partial \operatorname{Div}(Y, d)}{\partial y_i}
$$
&ensp;Then we walk through the activation function:
$$
\frac{\partial D i v}{\partial z_1^{(N)}}=\frac{\partial y_1^{(N)}}{\partial z_1^{(N)}} \frac{\partial D i v}{\partial y_1^{(N)}}=f_N^{\prime}\left(z_1^{(N)}\right) \frac{\partial D i v}{\partial y_1^{(N)}}
$$
<img src=".\image\HW1P1_image24.png" alt="image-20221224220206074" style="zoom:80%;" />

&ensp;We get the desired gradient on layer N, Yeah~. But it's not the time for cheering up, we have to move forward because we just computed one layer gradients, the backpropagation is still continuing. So we also have to calculate the derivative with respect to $y_i^{N-1}$.

<img src=".\image\HW1P1_image25.png" alt="image-20221224221058953" style="zoom:80%;" />

&ensp;Â Afer iteratively calculating gradients like this till the last layer, backpropagation is finished. So let me take a brief summary bellow:

<img src=".\image\HW1P1_image26.png" alt="image-20221224221204602" style="zoom:80%;" />

<img src=".\image\HW1P1_image27.png" alt="image-20221224213345087" style="zoom:80%;" />

&ensp;In our assumptions, the activation function is scale-wise and all the fucntions are differentiable, but we don't get the two conditions in many cases. So for vector activation function, we need to do one more summation, and instead of directly using the gradient, we can choose subgradients sometimes.

<img src=".\image\HW1P1_image28.png" alt="image-20221224230903495" style="zoom:80%;" />

&ensp;Â The matrix form is as the following, and i think the only equation needed to be clarify is the term $\grad_{W_N}Div$, note that the dim of the derivative of a scaler with respect to a vector or matrix is the same as its transpose dim. And the dim of this expression looks resonable right? <img src=".\image\HW1P1_image31.png" alt="image-20221225220955510" style="zoom:80%;" />

&ensp;Now i'm gonna show you the math in detail. Our convention is to multiply gradient on the left, so to calculate the derivative  $\grad_{W_N}z_N$, we need to transpose the expression $z_N = W_Ny_{N-1} + b_N$ to get $z_N^T = y_{N-1}^TW_N^T + b_N^T$. By applying the chain rule, we can get:
$$
\grad_{W_N^T}Div = \grad_{z_N^T}Div\grad_{W_N^T}z_N^T=\grad_{z_N^T}Div \ y_{N-1}^T
$$
 &ensp;Â Adding transpose, we get:
$$
\grad_{W_N}Div = y_{N-1}\grad_{z_N}Div
$$
<img src=".\image\HW1P1_image30.png" alt="image-20221225214643166" style="zoom:80%;" />

<img src=".\image\HW1P1_image29.png" alt="image-20221225214508846" style="zoom:80%;" />

> åšäº†CMUçš„å®éªŒä»¥åå‘ç°æŒ‡å¯¼ä¹¦å†™çš„å’Œè¯¾å ‚ä¸Šè®²çš„ä¸ä¸€æ ·XDï¼Œè¯¾ä¸Šè®²çš„æ˜¯åˆ†å­å¸ƒå±€ï¼Œå®éªŒæ˜¯åˆ†æ¯å¸ƒå±€ï¼Œæ‰€ä»¥ä¸‹é¢ä¼šæŠŠçŸ©é˜µæ±‚å¯¼è®²çš„å…¨ä¸€ç‚¹ï¼Œäº‰å–åˆ‡å‰²è¿™ä¸€èŠ‚ã€‚

å‘é‡ï¼ˆæˆ–è€…æ ‡é‡ï¼‰å¯¹å‘é‡çš„å¯¼æ•°å¾ˆç®€å•ï¼ŒJacobianå°±å¤Ÿäº†ï¼Œè¿™ä¸ªæˆ‘åœ¨HW0è®²çš„æ¯”è¾ƒç»†äº†ï¼Œä¸»è¦è¿˜æ˜¯è®¨è®ºä¸€ä¸‹çŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼ã€‚æ¥ä¸‹æ¥æˆ‘è®²çš„å†…å®¹åªæ˜¯é’ˆå¯¹æ·±åº¦å­¦ä¹ é‡Œçš„çŸ©é˜µæ±‚å¯¼ï¼Œå› ä¸ºå¯èƒ½ä¸åŒé¢†åŸŸå¯¹è¿™å—å®šä¹‰ä¸ä¸€æ ·ï¼Œæˆ‘ä¹Ÿä¸å¤ªæ‡‚hhã€‚

å¼•ç”¨bä¹å¤§ä½¬çš„å›ç­”ï¼Œç®—æ˜¯å¯¹çŸ©é˜µæ±‚å¯¼åšäº†ä¸€ä¸ªå®šä¹‰ï¼š

æ¥ä¸¾ä¸ªæ —å­å§ï¼Œ$A B=C$ï¼Œ$\left[\begin{array}{ll}a_1 & a_2 \\ a_3 & a_4\end{array}\right]\left[\begin{array}{ll}b_1 & b_2 \\ b_3 & b_4\end{array}\right]=\left[\begin{array}{ll}c_1 & c_2 \\ c_3 & c_4\end{array}\right]$
å…¶ä¸­
$$
\left\{\begin{array}{l}
c_1=a_1 b_1+a_2 b_3 \\
c_2=a_1 b_2+a_2 b_4 \\
c_3=a_3 b_1+a_4 b_3 \\
c_4=a_3 b_2+a_4 b_4
\end{array}\right.
$$
é‚£ä¹ˆ
$$
\frac{\partial C}{\partial A} \Rightarrow\left\{\begin{array}{c}
\partial c_1 / \partial a_1=b_1 \\
\partial c_1 / \partial a_2=b_3 \\
\partial c_1 / \partial a_3=0 \\
\partial c_1 / \partial a_4=0 \\
\partial c_2 / \partial a_1=b_2 \\
\vdots \\
\partial c_4 / \partial a_4=b_4
\end{array}\right.
$$
å…¶å®ç›¸å½“äºæ‰©å±•äº†Jacobiançš„å®šä¹‰ï¼Œå³$C$ä¸­æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¯¹äº$A$ä¸­æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ±‚å¯¼ã€‚è½¬åŒ–æˆæ ‡é‡çš„å½¢å¼å°±å¥½ç†è§£äº†å§~è‡³äºæŠŠä»¥ä¸Š16ä¸ªæ ‡é‡æ±‚å¯¼å†™æˆ$4 \times 4$çš„çŸ©é˜µä¹Ÿå¥½è¿˜æ˜¯16ç»´çš„å‘é‡ä¹Ÿå¥½ï¼Œå¤§å¤šæ˜¯ä¸ºäº†å½¢å¼ï¼ˆç†è®ºï¼‰ä¸Šçš„ç¾è§‚ï¼Œæˆ–æ˜¯æ–¹ä¾¿å¯¹æ±‚å¯¼ç»“æœçš„åç»­ä½¿ç”¨ï¼Œäº¦æˆ–æ˜¯æ–¹ä¾¿ç¼–ç¨‹å®ç°ï¼Œ**æŒ‰éœ€è‡ªå–**ï¼Œå…¶æœ¬è´¨ä¸å˜ã€‚

åœ¨ç¥ç»ç½‘ç»œé‡Œï¼Œæ‰€è°“çš„çŸ©é˜µå½¢å¼éƒ½æ˜¯é€šè¿‡æ ‡é‡è¿›è¡Œå½¢å¼åŒ–åŒ…è£…çš„ï¼Œå®é™…ä¸Šçš„æ±‚å¯¼è§„åˆ™è¿˜æ˜¯è¦å’Œæ ‡é‡å¯¼æ•°è¿›è¡Œå¯¹åº”ï¼Œå› ä¸ºå‰å‘ä¼ æ’­ä¸­ä¸åŒæ ·æœ¬ä¹‹é—´æ˜¯äº’ä¸å½±å“çš„ï¼ˆæš‚æ—¶ä¸è€ƒè™‘batch normç­‰ï¼‰ï¼Œä¹Ÿå°±æ˜¯åªè€ƒè™‘affine functionå’Œelement-wiseçš„æ¿€æ´»å‡½æ•°çš„æƒ…å†µï¼ˆvectoræ¿€æ´»å‡½æ•°å°±å•ç‹¬ç®—ä¸€ä¸‹å°±è¡Œï¼‰ï¼Œæ‰€ä»¥å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œæ•°å­¦ä¸Šçš„ç­”æ¡ˆåº”è¯¥æ˜¯$B^\top \otimes I$ï¼Œå…¶ä¸­ $\otimes$ æ˜¯kroneckerç§¯ï¼Œkronecker productçš„å®šä¹‰å¯ä»¥çœ‹è¿™é‡Œ [here]( https://zhuanlan.zhihu.com/p/457055092) ï¼Œå…·ä½“ä¸ºå•¥æ˜¯è¿™ç»“æœå¯ä»¥çœ‹çŸ©é˜µæ±‚å¯¼æœ¯ã€‚ä½†æ˜¯åœ¨ç¥ç»ç½‘ç»œé‡Œï¼Œæˆ‘ä»¬çš„ç­”æ¡ˆåªæ˜¯$B^\top$ï¼Œå› ä¸ºæœ¬è´¨ä¸Šæˆ‘ä»¬è¿˜æ˜¯åœ¨å¤„ç†ä¸€ä¸ªå¤šå…ƒå‡½æ•°çš„ä¼˜åŒ–ï¼ŒçŸ©é˜µçš„å½¢å¼åªæ˜¯ç»„ç»‡ç»“æœçš„ä¸€ç§æ–¹å¼ï¼Œæˆ‘ä»¬æŠŠå¤šä¸ªè®­ç»ƒæ ·æœ¬ç»„æˆä¸€æ‰¹å½¢æˆçŸ©é˜µï¼Œå®é™…ä¸Šå’Œå•ä¸ªæ ·æœ¬å‘é‡çš„å¤„ç†å·®åˆ«åªåœ¨äºéœ€è¦å¯¹ä¸åŒbatchæ ·æœ¬å¾—åˆ°çš„æ¢¯åº¦ç»“æœæ±‚å’Œï¼Œä¸‹é¢ä¸¾ä¸ªä¾‹å­ï¼ˆæ²¿ç”¨bä¹å¤§ä½¬çš„[ä¾‹å­](https://zhuanlan.zhihu.com/p/37916911)ï¼‰ï¼š

å‡è®¾batch sizeä¸º3ï¼Œå¯ä»¥å¾—åˆ°
$$
Y_{2 \times 3}=W_{2 \times 3} \cdot X_{3 \times 3}+B_{2 \times 1}
$$
å¯¹äºå…¶ä¸­çš„æŸä¸ªp sampleè€Œè¨€
$$
\begin{aligned}
& y_{1p}=w_{11} x_{1p}+w_{12} x_{2p}+w_{13} x_{3p}+b_1 \\
& y_{2p}=w_{21} x_{1p}+w_{22} x_{2p}+w_{23} x_{3p}+b_2
\end{aligned}
$$
ä»è€Œ
$$
\frac{\partial C}{\partial w_{i j}}=\sum_{p} \frac{\partial C}{\partial y_{ip}} \frac{\partial y_{ip}}{\partial w_{i j}}=\sum_{p} x_{jp} \frac{\partial C}{\partial y_{ip}}
$$
è¿™é‡Œçš„æ±‚å’Œå…¶å®å°±æ˜¯å…³é”®æ‰€åœ¨ï¼Œæˆ‘ä»¬æ˜¯åˆ©ç”¨batchè®¡ç®—å‡ºçš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°
$$
\begin{aligned}
\frac{\partial C}{\partial W} & =\left(\begin{array}{lll}
\sum_p x_{1p} \frac{\partial C}{\partial y_{1p}} & \sum_p x_{2p} \frac{\partial C}{\partial y_{1p}} & \sum_p x_{3p} \frac{\partial C}{\partial y_{1p}} \\
\sum_p x_{1p} \frac{\partial C}{\partial y_{2p}} & \sum_p x_{2p} \frac{\partial C}{\partial y_{2p}} & \sum_p x_{3p} \frac{\partial C}{\partial y_{2p}}
\end{array}\right) \\
& =\left(\begin{array}{c}
\frac{\partial C}{\partial y_{11}} & \frac{\partial C}{\partial y_{12}} & \frac{\partial C}{\partial y_{13}}\\
\frac{\partial C}{\partial y_{21}} & \frac{\partial C}{\partial y_{22}} & \frac{\partial C}{\partial y_{23}}
\end{array}\right) \cdot\left(\begin{array}{lll}
x_{11} & x_{21}  & x_{31} \\
x_{12} & x_{22} & x_{32} \\
x_{13} & x_{23} & x_{33} 
\end{array}\right) \\
& =\frac{\partial C}{\partial Y} \cdot X^T
\end{aligned}
$$
ä¸‹é¢æ±‚ $\frac{\partial C}{\partial X}$ :
$$
\begin{aligned}
& \because \frac{\partial C}{\partial x_{jp} }=\frac{\partial C}{\partial y_{1p}} \frac{\partial y_{1p}}{\partial x_{jp}}+\frac{\partial C}{\partial y_{2p}} \frac{\partial y_{2p}}{\partial x_{jp}}=\frac{\partial C}{\partial y_{1p}} w_{1 j}+\frac{\partial C}{\partial y_{2p}} w_{2 j} \\
& \therefore \frac{\partial C}{\partial X}=\left(\begin{array}{l}
\frac{\partial C}{\partial x_{11}} & \frac{\partial C}{\partial x_{12}} & \frac{\partial C}{\partial x_{31}}\\
\frac{\partial C}{\partial x_{21}} & \frac{\partial C}{\partial x_{22}} & \frac{\partial C}{\partial x_{32}}\\
\frac{\partial C}{\partial x_{31}} & \frac{\partial C}{\partial x_{32}} & \frac{\partial C}{\partial x_{33}}
\end{array}\right) \\

& =\left(\begin{array}{ll}
w_{11} & w_{21} \\
w_{12} & w_{22} \\
w_{13} & w_{23}
\end{array}\right) \cdot\left(\begin{array}{c}
\frac{\partial C}{\partial y_{11}} & \frac{\partial C}{\partial y_{12}} & \frac{\partial C}{\partial y_{13}}\\
\frac{\partial C}{\partial y_{21}} & \frac{\partial C}{\partial y_{22}} & \frac{\partial C}{\partial y_{23}}
\end{array}\right) \\
& =W^T \cdot \frac{\partial C}{\partial Y} \\
&
\end{aligned}
$$
labä¸­çš„ä¾‹å­æ˜¯ï¼š
$$
Z=A \cdot W^T+\iota \cdot b^T \quad \in \mathbb{R}^{N \times C_1}
$$
å¯¹äºä½œä¸šä¸Šçš„æ±‚å¯¼ç»“æœæ˜¯è¿™æ ·çš„ï¼Œå¯èƒ½å”¯ä¸€è¿˜éœ€è¦æƒ³æƒ³çš„å°±æ˜¯ç¬¬ä¸€ä¸ªå¼å­ï¼ŒæŒ‰é“ç†ä¸åº”è¯¥ç¬¬ä¸€é¡¹çš„$\frac{\partial L}{\partial Z}$åº”è¯¥åŠ ä¸Šè½¬ç½®ä¹ˆï¼ˆå› ä¸ºæ˜¯åˆ†æ¯å¸ƒå±€ï¼‰ï¼Œå®é™…ä¸Šå› ä¸ºé“¾å¼æ³•åˆ™ä¹Ÿæ˜¯æœ‰æ–¹å‘æ€§ï¼Œåˆ†æ¯å¸ƒå±€éƒ½æ˜¯ä¹˜åœ¨å·¦ä¾§çš„ï¼ˆè¯¦ç»†çš„è§£é‡Šçœ‹ä¸Šé¢çš„ä¾‹å­å’Œä¸‹é¢çš„æ€»ç»“ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæŠŠ$Z$è½¬ç½®ï¼Œå¾—åˆ°$WA^T$çš„å½¢å¼ï¼Œå†å»å¯¹$A^T$æ±‚å¯¼ï¼Œç„¶åå†æŠŠç­”æ¡ˆè½¬ç½®è¿‡æ¥å°±èƒ½å¾—åˆ°ç»“æœäº†ã€‚
$$
\begin{aligned}
& \frac{\partial L}{\partial A}=\left(\frac{\partial L}{\partial Z}\right) \cdot\left(\frac{\partial Z}{\partial A}\right)^T \quad \in \mathbb{R}^{N \times C_0} \\
& \frac{\partial L}{\partial W}=\left(\frac{\partial L}{\partial Z}\right)^T \cdot\left(\frac{\partial Z}{\partial W}\right) \quad \in \mathbb{R}^{C_1 \times C_0} \\
& \frac{\partial L}{\partial b}=\left(\frac{\partial L}{\partial Z}\right)^T \cdot\left(\frac{\partial Z}{\partial b}\right) \quad \in \mathbb{R}^{C_1 \times 1} \\
&
\end{aligned}
$$


For any linear equation of the kind $Z = AX + c$, the derivative of $Z$ with respect to $A$ is $X$. The derivative of $Z$ with respect to $X$ is $A^T$ . Also the derivative with respect to a transpose is the transpose of the derivative, so the derivative of $Z$ with respect to $X$ is $A^T$ but the derivative of $Z$ with respect to $X^T$ is $A$.

æ€»ç»“ä¸‹å°±æ˜¯:
$$
z=f(Y), Y=A X+B \rightarrow \frac{\partial z}{\partial X}=A^T \frac{\partial z}{\partial Y}
$$
è¿™ç»“è®ºåœ¨ $\mathbf{x}$ æ˜¯ä¸€ä¸ªå‘é‡çš„æ—¶å€™ä¹Ÿæˆç«‹, å³:
$$
z=f(\mathbf{y}), \mathbf{y}=A \mathbf{x}+\mathbf{b} \rightarrow \frac{\partial z}{\partial \mathbf{x}}=A^T \frac{\partial z}{\partial \mathbf{y}}
$$
å¦‚æœè¦æ±‚å¯¼çš„è‡ªå˜é‡åœ¨å·¦è¾¹, çº¿æ€§å˜æ¢åœ¨å³è¾¹, ä¹Ÿæœ‰ç±»ä¼¼ç¨æœ‰ä¸åŒçš„ç»“è®ºå¦‚ä¸‹, è¯æ˜æ–¹æ³•æ˜¯ç±»ä¼¼çš„, è¿™é‡Œç›´æ¥ç»™å‡ºç»“è®º:
$$
z=f(Y), Y=X A+B \rightarrow \frac{\partial z}{\partial X}=\frac{\partial z}{\partial Y} A^T
$$
$$
z=f(\mathbf{y}), \mathbf{y}=X \mathbf{a}+\mathbf{b} \rightarrow \frac{\partial z}{\partial \mathbf{X}}=\frac{\partial z}{\partial \mathbf{y}} a^T
$$

æœ€åè¿˜æ˜¯æä¸€å˜´ï¼Œåˆ«çŠ¯è¿·ç³Šäº†ï¼Œç®—å‡ºæ¥çš„æ¢¯åº¦æ˜¯ç”¨æ¥æ›´æ–°åŸå‚æ•°çš„ï¼Œä¹Ÿå°±æ˜¯ç›¸å½“äº$\Delta x$çš„æ„Ÿè§‰ï¼Œå› ä¸ºæ˜¯å¤šå…ƒå‡½æ•°ï¼Œæ‰€ä»¥æœ€åå¯¹ç›®æ ‡å‡½æ•°çš„å¢ç›Šåº”è¯¥æ˜¯$\grad grad \cdot \Delta x$ï¼Œä¹‹å‰å¤ªä¹…æ²¡çœ‹è¿™éƒ¨åˆ†æ™•äº†ä¸€æ¬¡ã€‚

> å‚è€ƒèµ„æ–™ï¼šhttps://github.com/FelixFu520/README/blob/main/train/optim/matrix_bp.md

### Optimization

> ç”¨è‹±æ–‡å†™å¥½ç´¯å“‡ï¼Œå¼€æ‘†äº†ï¼Œä»¥åä¸­è‹±æ··ç€å†™ï¼Œå¯èƒ½åªæœ‰è‡ªå·±çœ‹å¾—æ‡‚å§ğŸ¤£ï¼Œhhè¿˜æ˜¯è¦ä¿è¯é€šä¿—æ˜“æ‡‚

#### Material

å‚è€ƒèµ„æ–™ï¼š

http://deeplearning.cs.cmu.edu/F22/document/slides/lec6.optimization.pdf

http://deeplearning.cs.cmu.edu/F22/document/slides/lec7.stochastic_gradient.pdf

æ–‡ç« æ¡†æ¶æŒ‰ç…§ä¸‹é¢ä¸¤ä¸ªblogå™è¿°ï¼Œå…·ä½“çš„ç»†èŠ‚å’Œç»„ç»‡æ–¹å¼ä¼šä¸°å¯Œå¾ˆå¤šï¼š

https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/

#### Basic conceptions

é¦–å…ˆè¿˜æ˜¯æ¥ä»‹ç»ä¸€äº›åŸºç¡€çš„æ¦‚å¿µå’Œideaï¼Œä¼šä»æœ€ç®€å•çš„æ¢¯åº¦ã€æµ·å¡çŸ©é˜µã€æ³°å‹’å±•å¼€è°ˆèµ·ï¼Œæˆ‘ä¼šåˆ†äº«æˆ‘çš„ä¸€äº›æ€è€ƒæ–¹å¼ã€‚ç„¶åä¼šä»‹ç»äºŒæ¬¡ä¼˜åŒ–çš„æ–¹æ³•ï¼Œå†è¿‡æ¸¡åˆ°ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–æ–¹æ³•ã€‚

æ¢¯åº¦èƒ½åæ˜ å¢é•¿æœ€å¿«çš„æ–¹å‘ï¼Œè¿™ä¸ªå¯ä»¥ä»hyperplaneçš„è§’åº¦å»ç†è§£ï¼Œå¯ä»¥å‚è€ƒbä¹çš„è¿™ä¸ªå›ç­” [here](https://www.zhihu.com/question/36301367/answer/198887937)ã€‚äºŒé˜¶å¯¼é™¤äº†å¯ä»¥ç†è§£ä¸ºå¯¼æ•°çš„å¢é•¿ç‡ï¼Œè¿˜å¯ä»¥ä»åŸå‡½æ•°å±€éƒ¨å‡å€¼çš„è§’åº¦ç†è§£ï¼Œä¹Ÿå°±æ˜¯**Laplace** ç®—å­çš„è§’åº¦ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªè§†é¢‘ [here](https://www.youtube.com/watch?v=JQSC0lCPG24&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=68)ã€‚å¤šå…ƒå‡½æ•°çš„æ³°å‹’å±•å¼€å¯ä»¥å†™ä½œçŸ©é˜µä¹˜æ³•çš„å½¢å¼å…¶å®ä¹Ÿæ˜¯ä»scaleæ¨çš„ï¼ŒäºŒé˜¶çš„å±•å¼€æ¨å¯¼å¯ä»¥çœ‹è¿™é‡Œ [here](https://zhuanlan.zhihu.com/p/33316479)ã€‚

æ¥ä¸‹æ¥å°±æ˜¯å’Œå‡¸ä¼˜åŒ–æœ‰å…³çš„ç†è®ºäº†ï¼Œå¯¹æœ€ç®€å•çš„äºŒæ¬¡ä¼˜åŒ–ï¼Œå…¶å®ç‰›é¡¿æ³•å°±å¯ä»¥ç»™å‡ºæ¯æ¬¡æ›´æ–°çš„æœ€ä¼˜æ–¹å‘å’Œæ­¥é•¿ï¼Œå…¶å®ä¹Ÿå°±æ˜¯æ³°å‹’äºŒé˜¶å±•å¼€åï¼Œè®¡ç®—å¾—åˆ°çš„æœ€ä¼˜å€¼ï¼Œä¸€ç»´scaleçš„æƒ…å†µæ—¶ï¼Œè¿™ä¸ªæœ€ä¼˜çš„å­¦ä¹ ç‡å…¶å®å°±æ˜¯äºŒé˜¶å¯¼çš„å€’æ•°ï¼Œè€Œå¯¹äºå¤šå…ƒå˜é‡ï¼Œå°±å˜æˆäº†æµ·å¡çŸ©é˜µçš„é€†äº†ã€‚

æœ‰å…³Lipschitzå¹³æ»‘ã€å¼ºå¯¹å¶çš„ç†è®ºç›´è§‚ä¸Šçš„æ„Ÿå—å¯ä»¥çœ‹è¿™é‡Œ[here](https://zhuanlan.zhihu.com/p/27554191)ï¼Œå’Œæ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§è¯æ˜çš„æœ‰å…³ç†è®ºå¯ä»¥çœ‹è¿™ä¸ª [here](https://cs.mcgill.ca/~wlh/comp451/files/comp451_chap8.pdf)ï¼Œè¿˜æœ‰è¿™é‡Œ [here](https://www.cs.ubc.ca/~schmidtm/SVAN16/L4.pdf) (åŠ æ‹¿å¤§çš„å­¦æ ¡çš„æ•™å­¦èµ„æ–™æ˜¯çœŸå¥½å•Š)ï¼Œè¯¦ç»†çš„æˆ‘å°±ä¸ä»‹ç»äº†ã€‚å¤§æ¦‚å°±æ˜¯è¯´å¼ºçªçš„æ”¶æ•›çš„ä¼šæ›´å¿«ï¼Œå¦‚æœåªæ˜¯Lipschitzå¹³æ»‘ä¹Ÿèƒ½æ”¶æ•›ã€‚

<img src=".\image\HW1P1_image32.png" alt="image-20230107230430281" style="zoom:80%;" />

<img src=".\image\HW1P1_image33.png" alt="image-20230107230509280" style="zoom:80%;" />

ç°åœ¨æ¥è®²è®²æµ·å¡çŸ©é˜µç‰¹å¾å€¼å’Œæ”¶æ•›çš„å…³ç³»ï¼Œæˆ‘ä»¬çŸ¥é“é€šè¿‡äºŒé˜¶å±•å¼€å¯ä»¥å¾—åˆ°å±€éƒ¨çš„è¿‘ä¼¼ï¼Œè¿™ä¸ªè¿‘ä¼¼åœ¨å±€éƒ¨æœ€å€¼é™„è¿‘çš„ç­‰å€¼çº¿ä½“ç°ä¸ºä¸€ä¸ªæ¤­åœ†ï¼Œè€Œè¿™ä¸ªæ¤­åœ†çš„é•¿çŸ­è½´å°±æ˜¯å’Œæµ·å¡çŸ©é˜µçš„ç‰¹å¾å€¼æˆæ¯”ä¾‹ï¼ŒåŸå› è¿™æ ·çš„ï¼Œé€šè¿‡æ³°å‹’å±•å¼€æˆ‘ä»¬å¯ä»¥æŠŠå‡½æ•°è¿‘ä¼¼å†™ä¸ºäºŒæ¬¡å‹çš„è¡¨è¾¾å¼ï¼Œå¯¹åº”çš„ç­‰å€¼çº¿å°±æ˜¯äºŒæ¬¡æ›²çº¿è¿™ç§ï¼Œé•¿çŸ­è½´å’Œç‰¹å¾å€¼çš„å…³ç³»å°±å¾ˆæ˜æ˜¾äº†ï¼Œé€šè¿‡SVDçš„å‡ ä½•æ„ä¹‰ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°ã€‚å¦‚æœæµ·å¡çŸ©é˜µçš„æ¡ä»¶æ•°å¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯æ¤­åœ†çš„é•¿çŸ­è½´ç›¸å·®å¾ˆå¤§ï¼Œè¯´æ˜è¿™ä¸ªæŸå¤±å‡½æ•°å¹³é¢æ˜¯ç—…æ€çš„ï¼Œå°±æ¯”è¾ƒéš¾æ”¶æ•›ã€‚



<img src=".\image\HW1P1_image34.png" alt="image-20230107222504865" style="zoom:80%;" />

<img src=".\image\HW1P1_image35.png" alt="image-20230107222557585" style="zoom:80%;" />



ä½†æ˜¯æµ·å¡çŸ©é˜µçš„é€†è®¡ç®—ä»£ä»·å¤§æ¦‚æ˜¯$O(N^3)$çš„ï¼Œæ”¾åˆ°ç¥ç»ç½‘ç»œé‡Œé¢è‚¯å®šæ˜¯å«©ç®—ç®—ä¸å‡ºæ¥çš„ï¼Œä¹Ÿå°±æ˜¯è¯´å¯¹å‚æ•°çš„æ¯ä¸ªåˆ†é‡ç¡®å®šæœ€ä¼˜çš„å­¦ä¹ ç‡æ˜¯å¾ˆéš¾çš„ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸¤ç§ä¸åŒçš„åšæ³•ï¼Œç¬¬ä¸€ç§è¿˜æ˜¯é‡‡å–æ¯ä¸ªåˆ†é‡ç‹¬ç«‹çš„å­¦ä¹ ç‡ï¼Œä½†æ˜¯ä½¿ç”¨å¯å‘å¼çš„æ€è·¯è€Œä¸æ˜¯å«©ç®—ï¼ˆRpropç®—æ³•ï¼‰ï¼Œç¬¬äºŒç§å°±æ˜¯æ‰€æœ‰åˆ†é‡ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œå°±æ˜¯å¸¸è§çš„å­¦ä¹ ç‡ç›´æ¥ä¹˜æ¢¯åº¦å‘é‡ï¼Œä½†æ˜¯è¿™æ ·çš„è¯æ”¶æ•›æ€§å°±ä¸ä¸€å®šèƒ½ä¿è¯äº†ï¼Œä»¥äºŒæ¬¡ä¼˜åŒ–ä¸ºä¾‹ï¼Œå¦‚æœå­¦ä¹ ç‡å¤§äºä»»ä¸€åˆ†é‡æœ€ä¼˜å­¦ä¹ ç‡çš„äºŒå€ï¼Œå°±ä¼šå‘æ•£ï¼Œä¹Ÿç”±æ­¤æå‡ºäº†å­¦ä¹ ç‡è¡°å‡å’Œæ”¶æ•›çš„ç†è®ºï¼ŒRobbins-Munroe conditionsç®—æ˜¯æœ€ç»å…¸çš„æ”¶æ•›æ¡ä»¶ï¼Œä¹Ÿå°±æ˜¯SGDå¯¹äºæ»¡è¶³å‡¸æ€§å’Œå¹³æ»‘çš„æ¨¡å‹ï¼Œåªè¦å­¦ä¹ ç‡æ»¡è¶³ä¸‹é¢çš„æ¡ä»¶å°±ä¼šæ”¶æ•›
$$
\sum_{k=0}^{\infty} \alpha^{(k)}=\infty \quad \sum_{k=0}^{\infty}\left(\alpha^{(k)}\right)^2<\infty
$$
æ­¤å¤–åœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­è¿˜å­˜åœ¨ä¸»è¦çš„ä¸¤ä¸ªé—®é¢˜éœ€è¦è§£å†³ï¼Œé’ˆå¯¹è¿™ä¸¤ä¸ªé—®é¢˜æå‡ºçš„å„ç§è§£å†³æ–¹æ³•å°±æ˜¯è¿™èŠ‚çš„é‡ç‚¹å­¦ä¹ å†…å®¹äº†ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯è¾¾ä¸åˆ°å…¨å±€æœ€å€¼ï¼Œä¼šæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œç¬¬äºŒä¸ªé—®é¢˜æ˜¯å¯¹äºç—…æ€çš„æŸå¤±å‡½æ•°å¹³é¢å¾ˆå¯èƒ½ä¼šå‡ºç°éœ‡è¡çš„ç°è±¡ï¼Œå¯¼è‡´æ”¶æ•›æ…¢æ•ˆæœå¾ˆå·®ã€‚



#### SGD and Batch

ç›´æ¥ç”¨å…¨æ ·æœ¬å»è®¡ç®—loss functionä¼šæœ‰è‡³å°‘ä¸¤ä¸ªé—®é¢˜ï¼Œç¬¬ä¸€ä¸ªæ˜¯cycleçš„è¡¨ç°ï¼Œç¬¬äºŒä¸ªæ˜¯ä¸èƒ½é¿å…å±€éƒ¨æœ€å€¼ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥åˆ©ç”¨éšæœºé‡‡æ ·è®¡ç®—æ¢¯åº¦ï¼Œæœ€ç®€å•çš„ç­–ç•¥å°±æ˜¯éšæœºå–ä¸€ä¸ªæ ·æœ¬ï¼Œè¿™æ˜¯SGDçš„æ€è·¯ï¼Œä½†æ˜¯è¿™æ ·ä¼šä½¿æ”¶æ•›è¿‡ç¨‹çš„æ–¹å·®å˜å¾—å¾ˆå¤§ï¼Œè€Œä¸”æŸå¤±å‡½æ•°ä¹Ÿä¸ä¼šå˜å¾—è¶³å¤Ÿå°ã€‚

<img src=".\image\HW1P1_image36.png" alt="image-20230108115028332" style="zoom:80%;" />



<img src=".\image\HW1P1_image37.png" alt="image-20230108115100729" style="zoom:80%;" />

é’ˆå¯¹è¿™ä¸ªé—®é¢˜ä¹Ÿæ˜¯æå‡ºäº†mini-batchå»è§£å†³ï¼ŒåŒæ ·ä½œä¸ºæ— åä¼°è®¡ï¼Œä½†æ˜¯å…¶æ–¹å·®ç¼©å°ä¸ºåŸæœ¬çš„${1\over batch}$å€ï¼Œæ”¶æ•›çš„æ•ˆæœä¹Ÿæ˜¯å¾—åˆ°äº†æå‡ï¼š

<img src=".\image\HW1P1_image38.png" alt="image-20230108115150825" style="zoom:80%;" />

<img src=".\image\HW1P1_image39.png" alt="image-20230108115259488" style="zoom:80%;" />

<img src=".\image\HW1P1_image40.png" alt="image-20230108115336885" style="zoom:80%;" />

#### Learning Rate and Grad direction

é’ˆå¯¹ç—…æ€çš„éœ‡è¡æƒ…å†µï¼Œå› ä¸ºæˆ‘ä»¬åªèƒ½åˆ©ç”¨ä¸€é˜¶å¯¼çš„ä¿¡æ¯ï¼Œå¦‚æœå¯ä»¥åˆ©ç”¨äºŒé˜¶å¯¼å…¶å®å¯ä»¥è·å¾—æ›²ç‡ç­‰ä¿¡æ¯è¿›è¡Œé¿å…ï¼Œä½†æ˜¯ç”±äºè®¡ç®—é‡æˆ‘ä»¬è¿˜æ˜¯æƒ³ä¸€äº›å¯å‘å¼çš„æ–¹æ³•å»è§£å†³ï¼Œæœªæ¥çš„æ”¹è¿›æ–¹å‘è‚¯å®šå°±æ˜¯ä½•å¦‚å…¼é¡¾äºŒé˜¶ä¿¡æ¯åˆå‡å°‘è¿ç®—é‡ã€‚

è¿™éƒ¨åˆ†è¯´å®è¯èµ„æ–™éƒ½å¾ˆå…¨äº†ï¼Œå€¼å¾—æä¸€å˜´çš„æ˜¯åŸºäºå­¦ä¹ ç‡æ”¹è¿›çš„ç®—æ³•ï¼Œ[AdaGrad](https://zhuanlan.zhihu.com/p/29920135) å’Œ RMS Propçš„å…³ç³»ï¼ŒRMS Propé€šè¿‡ç§»åŠ¨å¹³å‡è§£å†³äº†Adagradä¸­å¹³æ–¹å’Œç´¯åŠ è¿‡å¤§ç¼ºä¹æ­£åˆ™åŒ–çš„é—®é¢˜ã€‚

### Normalization

<img src=".\image\HW1P1_image41.png" alt="image-20230108145934082" style="zoom:80%;" />

<img src=".\image\HW1P1_image42.png" alt="image-20230108150349572" style="zoom:80%;" />

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨batch-normçš„è¯ ï¼Œçº¿æ€§å±‚å°±ä¸éœ€è¦é¢å¤–çš„biasé¡¹äº†ï¼Œä¼šè¢«å½’ä¸€åŒ–æ‰ï¼Œç®—æ˜¯ä¸ªArbitraryçš„é€‰æ‹©ï¼Œå¯åŠ å¯ä¸åŠ ã€‚

<img src=".\image\HW1P1_image43.png" alt="image-20230108150531429" style="zoom:80%;" />





## Python Implementation

è¿™éƒ¨åˆ†ç›´æ¥çœ‹æˆ‘ä»“åº“å§ï¼Œå°±ä¸æ”¾åˆ°è¿™é‡Œå†™äº†ï¼š[code](https://github.com/XuRui314/CMU-11-785)



## Torch Pipeline

&ensp;Colloquially,training a model can be described like this:

1. We get data-pairs of questions and answers.

2. For a pair `(x, y)`, we run `x` through the model to get the model's answer `y`. 
3. Then, a "teacher" gives the model a grade depending on â€œhow wrongâ€ `y`  is compared to the true answer `y`.
4. Then based on the grade,we figure out who's fault the error is.
5. Then, we fix the faults so the model can do better next time.

&ensp;To train a model using Pytorch, in general, there are 5 main parts:

1. Data
2. Model
3. Loss Function
4. Backpropagation
5. Optimizer

### Data

&ensp;When training a model, data is generally a long list of `(x, y)` pairs, where you want the model to see `x` and predict `y`.

&ensp;Pytorch has two classes you will need to use to deal with data:

- `torch.utils.data.Dataset` 
- `torch.utils.data.Dataloader`

&ensp;Dataset class is used to preprocess data and load single pairs `(x, y)`

&ensp;Dataloader class uses your Dataset class to get single pairs and group them into batches

<img src=".\image\HW1P1_image7.png" alt="image-20221215214237507" style="zoom:80%;" />



&ensp;When defining a Dataset, there are three class methods that you need to implement 3 methods: `__init__`, `__len__`, `__getitem__`.

&ensp;Use `__init__` to load the data to the class so it can be accessed later, Pytorch will use `__len__` to know how many `(x, y)` pairs (training samples) are in your dataset. After using `__len__` to figure out how many samples there are, Pytorch will use `__getitem__` to ask for() a certain sample. So `__getitem__(i)` should return the "i-th" sample, with order chosen by you. You should use `getitem` to do some final processing on the data before itâ€™s sent out. Since `__getitem__` will be called maybe millions of times, so make sure you do as little work in here as possible for fast code. Try to keep heavy preprocessing in `__init__`, which is only called once

&ensp;Here is a simple Dataset example:

```python
class MyDataset(data.Dataset):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y
      
    def __len__(self.Y):
        return len(self.Y)
    
    def __getitem(self, index):
        X = self.X[index].float().reshape(-1) #flatten the input
        Y = self.Y[index].long()
        return X, Y
```

&ensp;Here is a simple Dataloader example:

```python
# Training
train_dataset = MyDataset(train.train_data, train.train_labels)

train_loader_args = dict(shuffle = True, batch_size = 256, num_workers = num_workers, pin_memory = True)\
if cuda else dict(shuffle = True, batch_size = 64)

train_loader = data.DataLoader(train_dataset, **train_loader_args)
```

### Model

&ensp;This section will be in two parts:

â€¢ How to generate the model youâ€™ll use

â€¢ How to run the data sample through the model.

<img src=".\image\HW1P1_image8.png" alt="image-20221215224941804" style="zoom:80%;" />

&ensp;One key point in neural network is modularity, this means when coding a network, we can break down the structure into small parts and take it step by step.

&ensp;Now, letâ€™s get into coding a model in Pytorch. Networks in Pytorch are (generally) classes that are based off of the `nn.Module class`. Similar to the Dataset class, Pytorch wants you to implement the `__init__ ` and `forward` methods.

â€¢ `__init__`: this is where you define the actual model itself (along with other 

stuff you might need)

â€¢ `Forward`: given an input `x`, you run it through the model defined in `__init__`

```python
class Our_Model(nn.Module):
	def __init__(self):
        super().__init__()
        
        self.layer1 = nn.Linear(3, 4)
        self.layer2 = nn.Linear(4, 4)
        self.layer3 = nn,Linear(4, 1)
      
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.layer3(out)
      
    	return out
```

&ensp;However, it can get annoying to type each of the layers twice â€“ once in `__init__ `and once in forward. Since on the right, we take the output of each layer and directly put it into the next, we can use the **nn.Sequential** module.

```python
class Our_Model(nn.Module):
    def __init__(self):
        layers = {
            nn.Linear(3, 4),
            nn.Linear(4, 4),
            nn.Linear(4, 1)
        }
        self.layers = nn.Sequential(*layers) # * operator opens up the lsit and directly puts them in as arguments of nn.Sequential
        
	def forward(self, x):
        return self.layers(out)
```

&ensp;As a beginner to Pytorch, you should definitely have [link](https://pytorch.org/docs/stable/nn.html) open. The documentation is very thorough. Also, for optimizers: [link](https://pytorch.org/docs/stable/optim.html).

&ensp;Now that we have our model generated, how do we use it? First, we want to put the model on GPU. Note that for `nn.Module` classes, `.to(device)` is in-place. However, for tensors, you must do `x = x.to(device)`

```python
device = torch.device("cuda" if cuda else "cpu")
model.to(device)
```

&ensp;Also, models have `.train()` and `.eval()` methods. Before training, you should run `model.train()` to tell the model to save gradients. When validating or testing, run `model.eval()` to tell the model it doesnâ€™t need to save gradients (save memory and time). A common mistake is to forget to toggle back to .train(), then your model doesnâ€™t learn anything.

&ensp;So far, we can build:

```python
# Dataset Stuff
train_dataset = MyDataset(train.train_data, train.train_labels)

train_loader_args = dict(shuffle = True, batch_size = 256, num_workers = num_workers, pin_memory = True)\
if cuda else dict(shuffle = True, batch_size = 64)

train_loader = data.DataLoader(train_dataset, **train_loader_args)

# Model Stuff
model = nn.Sequential(nn.Linear(3,4 ), nn.Linear(4, 4), nn.Linear(4, 1))
device = torch.device["cuda" if cuda else "cpu"]
model.to(device)

# Optimization Stuff
NUM_EPOCHS = 100
# ----------------- #
# |	Not	cover yet | # initialize the criterion
# ----------------- #

# Training
for epoch in range(NUM_EPOCHS):
    model.train()
    
    for(x, y) in train_loaders:
        # ----------------- #
		# |	Not	cover yet | # optimizer initialize
		# ----------------- #
        x.to(device)
        y.to(device)
        
        output = model(x)
        # ----------------- #
		# |	Not	cover yet | # calculate loss, minimize this loss and backpropagate
		# ----------------- #
```

### Loss Function

To recap, we have run x through our model and gotten â€œoutput,â€ or `y`. Recall we need something to tell us how wrong it is compared to the true answer `y`. We rely on a â€œloss function,â€ also called a â€œcriterionâ€ to tell us this. The choice of a criterion will depend on the model/application/task,  but for classification, a criterion called â€œCrossEntropyLossâ€ is commonly used.

```python
# Optimization Stuff
NUM_EPOCHS = 100
criterion = nn.CrossEntropyLoss()
# ----------------- #
# |	Not	cover yet | #
# ----------------- #

#Training
for epoch in range(NUM_EPOCHS):
    model.train()
    
    for(x, y) in train_loaders:
        # ----------------- #
		# |	Not	cover yet | #
		# ----------------- #
        x.to(device)
        y.to(device)
        
        output = model(x)
        loss = criterion(output, y)
        
        # ----------------- #
		# |	Not	cover yet | #
		# ----------------- #
```





### Backpropagation

Backpropagation is the process of working backwards from the loss and calculating the gradients of every single (trainable) parameter w.r.t the loss. The gradients tell us the direction in which to move to minimize the loss.

```python
#Training
for epoch in range(NUM_EPOCHS):
    model.train()
    
    for(x, y) in train_loaders:
        # ----------------- #
		# |	Not	cover yet | #
		# ----------------- #
        x.to(device)
        y.to(device)
        
        output = model(x)
        loss = criterion(output, y)
        
        loss.backward() # add here
        # ----------------- #
		# |	Not	cover yet | #
		# ----------------- #
```

By doing `loss.backward()`, we get gradients w.r.t the loss. Remember model.train()? That allowed us to compute the gradients. If it had been in the eval state, we wouldnâ€™t be able to even compute the gradients, much less train.



### Optimizer

Now, backprop only *computes* the $âˆ‡p$ values â€“ it doesnâ€™t do anything with them. We want to *update* the value of $p$ using $âˆ‡p$. This is the optimizerâ€™s job.

A crucial component of any optimizer is the â€œlearning rate.â€ This is a hyperparameter that controls how much we should believe in $âˆ‡p$.  Again, this will be covered in more detail in a future lecture. Ideally, $âˆ‡p$ is a perfect assignment of blame w.r.t the **entire** dataset. However, itâ€™s likely that optimizing to perfectly match the *current* (x, y) sample $âˆ‡p$ was generated from wonâ€™t be great for matching the entire dataset.

Among other concerns, the optimizer *weights* the $âˆ‡p$ with the learning rate and use the weighted âˆ‡p to update $p$. 



```python
# Optimization Stuff
NUM_EPOCHS = 100
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = 1e-4) # add here
#Training
for epoch in range(NUM_EPOCHS):
    model.train()
    
    for(x, y) in train_loaders:
        optimizer.zero_grad()
        x.to(device)
        y.to(device)
        
        output = model(x)
        loss = criterion(output, y)
        
        loss.backward() # add here
        optimizer.step()
```

What is zero_grad? Every call to .backward() saves gradients for each parameter in the model. However, calling `optimizer.step()` **does not** delete these gradients after using them. So, you want to remove them so they donâ€™t interfere with the gradients of the next sample.

By doing `optimizer.step()`, we update the weights of the model using the computed gradients.

After here, you would generally perform validation (after every epoch or a couple), to see how your model performs on data it is not trained on. Validation follows a similar format as training, but without `loss.backward()` or `optimizer.step()`. You should  check the notebooks for more guidance.

> The complete code: [link](https://colab.research.google.com/drive/1huAQcxM9jMqSNb4h6XJ78Xd8EM1-UF_x#scrollTo=Sg8IUZ1er0dl)



## End

è¿™ä¸ªChapterå°±å½“æˆé—²è¯åŒºå§ï¼Œçœ‹ç‘å…‹è«è’‚çš„æ—¶å€™çªç„¶æƒ³åˆ°äº†è–›å®šè°”çš„çŒ«ï¼ˆè¿™ä¸ªå®éªŒæƒ³è¯´æ˜çš„ç‚¹å…¶å®æ˜¯åé©³å“¥æœ¬å“ˆæ ¹æ´¾ï¼Œæˆ‘åªæ˜¯çªç„¶æƒ³åˆ°äº†hhï¼Œå’Œå®ƒçš„æœ¬æ„ä¸å¤ªä¸€æ ·ï¼‰ï¼Œç„¶åè”æƒ³åˆ°ä¹‹å‰å­¦æ¦‚ç‡è®ºæƒ³åˆ°çš„é—®é¢˜ï¼Œç°å®ä¸­å¾ˆå¤šæ—¶å€™éƒ½æ˜¯äº‹æƒ…çš„ç»“æœå·²ç»ç¡®å®šä¸‹æ¥ï¼Œè€Œæˆ‘ä»¬ä¸çŸ¥é“ï¼Œåªèƒ½ç”¨æ¦‚ç‡å»å»ºæ¨¡ï¼Œæ— è®ºæ€ä¹ˆå»ºæ¨¡æ€ä¹ˆè¯´éƒ½æŒºreasonableçš„ï¼Œäºå…¶è¯´æ˜¯é¢„çŸ¥æœªçŸ¥çš„äº‹ä»¶ï¼Œä¸å¦‚è¯´æ˜¯åœ¨å­˜åœ¨ä¿¡æ¯å·®çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†æã€‚æ‰€ä»¥æ‰éœ€è¦è´å¶æ–¯è¿™ç§ä¸æ–­æ›´æ–°è·å–åéªŒçš„æ–¹æ³•ï¼Œä»¥åŠå„ç§æ¶ˆé™¤information gapçš„ç­–ç•¥å§ï¼Œæˆ–è€…æœ‰æ²¡æœ‰ä»€ä¹ˆæ ¹æ®å› æœæº¯æºä¸€æ ·çš„å»ºæ¨¡æ‰‹æ®µï¼Œèƒ½æ›´åŠ ç›´æ¥çš„å¯¹æ¦‚ç‡è¿›è¡Œæ›´æ–°ï¼Œæƒ³æƒ³å¯èƒ½åœ¨ç°å®ä¸­ä¹Ÿæ˜¯intractableçš„ã€‚







